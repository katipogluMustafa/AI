{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt                # For Plotting\n",
    "import numpy as np                             # For Environment Operations\n",
    "import random                                  # For Genetic Algorithm  \n",
    "import imageio                                 # For video recording actions\n",
    "import gym                                     # The General Framework of the environment\n",
    "import math                                    # Distance Calculations \n",
    "import copy                                    # Deep and shallow copies\n",
    "import os                                      # Write maze runner records to disk\n",
    "import pandas as pd                            # Data Analysis\n",
    "import sys                                     # Flush stdout\n",
    "from IPython.display import Image as Img       # Show Gifs of the Records in Notebook\n",
    "from pathlib import Path                       # Image, Record paths\n",
    "from mazelab import BaseMaze                   # The Framework of the environment\n",
    "from mazelab import Object                  \n",
    "from mazelab import DeepMindColor as color\n",
    "from mazelab import BaseEnv                    # Base Environment Class\n",
    "from mazelab import VonNeumannMotion           # North, South, West, East Motions\n",
    "from gym.envs.classic_control import rendering # For rendering images\n",
    "from gym.spaces import Box                     \n",
    "from gym.spaces import Discrete\n",
    "from PIL import Image                          # Rendering images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Env(BaseEnv):\n",
    "    \"\"\"\n",
    "    The environment of the Maze Solver\n",
    "    \n",
    "    Defines basics like stepping, rendering maze to image and also supports video recording with the help of BaseEnv\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Set Maze\n",
    "        self.maze = maze_env.get_maze()\n",
    "        # Define Observation Space\n",
    "        self.observation_space = Box(low=0, high=len(self.maze.objects), shape=self.maze.size, dtype=np.uint8)\n",
    "        # Set Actions\n",
    "        self.motions = VonNeumannMotion()\n",
    "        # Define Action Space\n",
    "        self.action_space = Discrete(len(self.motions))\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Take action on the environment\n",
    "        \"\"\"\n",
    "        # Take the motion that the action points\n",
    "        motion = self.motions[action]                      # North, South, West, East Motions\n",
    "        # Take the agent's current position\n",
    "        current_position = self.maze.objects.agent.positions[0]\n",
    "        # Calculate new position after we take the action\n",
    "        new_position = [current_position[0] + motion[0], current_position[1] + motion[1]]\n",
    "        # check whether the the new position is a wall or something alike\n",
    "        valid = self._is_valid(new_position)\n",
    "        \n",
    "        # If the result of the action to be taken is a valid position, then update the agent's position\n",
    "        if valid:\n",
    "            self.maze.objects.agent.positions = [new_position]\n",
    "        \n",
    "        # Since we use Reinforcement Learning, we give rewards to actions\n",
    "        reward = 0\n",
    "        # If the goal position is reached\n",
    "        if self._is_goal(new_position): \n",
    "            reward = 10_000                \n",
    "            done = True\n",
    "        # If the position is wall\n",
    "        elif not valid:\n",
    "            reward = -100\n",
    "            done = False\n",
    "        # Since every step takes time, \n",
    "        # we give punishment for every step taken to make the agent minimize n_steps\n",
    "        else:\n",
    "            reward = -10\n",
    "            done = False\n",
    "        # Return state,reward, done, info\n",
    "        return self.s, reward, done, {}\n",
    "    \n",
    "    \n",
    "    def encode(self, agent_x, agent_y):\n",
    "        \"\"\"\n",
    "        Change the state of the environment \n",
    "        into another by moving the agent to given position\n",
    "        \"\"\"\n",
    "        self.maze.objects.agent.positions[0] = [agent_x, agent_y]\n",
    "        return self.s\n",
    "    \n",
    "    def get_maze(self):\n",
    "        \"\"\"\n",
    "        Get maze in matrix form.\n",
    "        Here 2 shows the agent's position, \n",
    "        3 shows the goal position and 1s shows the walls\n",
    "        \"\"\"\n",
    "        return self.maze.to_value()\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Reset the Maze to the initial state\n",
    "        \"\"\"\n",
    "        self.maze.objects.agent.positions = maze_env.start_idx\n",
    "        self.maze.objects.goal.positions = maze_env.goal_idx\n",
    "        return self.s\n",
    "    \n",
    "    def _is_valid(self, position):\n",
    "        \"\"\"\n",
    "        Is given position valid? or the position have some obstacle?\n",
    "        \"\"\"\n",
    "        nonnegative = position[0] >= 0 and position[1] >= 0\n",
    "        within_edge = position[0] < self.maze.size[0] and position[1] < self.maze.size[1]\n",
    "        passable = not self.maze.to_impassable()[position[0]][position[1]]\n",
    "        return nonnegative and within_edge and passable\n",
    "    \n",
    "    def _is_goal(self, position):\n",
    "        \"\"\"\n",
    "        Have we reached to the goal position?\n",
    "        \"\"\"\n",
    "        out = False\n",
    "        goal_position = self.maze.objects.goal.positions[0]\n",
    "        if goal_position[0] == position[0] and goal_position[1] == position[1]:\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def get_image(self):\n",
    "        \"\"\"\n",
    "        Convert the maze to RGB Array so that we can print it out\n",
    "        \"\"\"\n",
    "        return self.maze.to_rgb()\n",
    "    \n",
    "    def render(self, mode='human', max_width=500):\n",
    "        \"\"\"\n",
    "        Print the maze as image of max_width length.\n",
    "        \n",
    "        Here careful to max_width, it can make the image blurry if not equal to the actual size of the image \n",
    "        \"\"\"\n",
    "        # Get the image of the maze\n",
    "        img = self.get_image()\n",
    "        # Conver the image into RGB array\n",
    "        img = np.asarray(img).astype(np.uint8)\n",
    "        # Scale up the image by 10 without loosing any quality\n",
    "        # Here be careful, for big mazes, 10 may be high\n",
    "        img = Env.repeat_upsample(img,10, 10)\n",
    "        # Return the image\n",
    "        img = np.asarray(img)\n",
    "        if mode == 'rgb_array':\n",
    "            return img\n",
    "        elif mode == 'human':\n",
    "            from gym.envs.classic_control.rendering import SimpleImageViewer\n",
    "            if self.viewer is None:\n",
    "                self.viewer = SimpleImageViewer()\n",
    "            self.viewer.imshow(img)\n",
    "            \n",
    "            return self.viewer.isopen\n",
    "        \n",
    "    @property\n",
    "    def s(self):\n",
    "        \"\"\"\n",
    "        Get the state of the maze\n",
    "        Every state is a mapping from agent's position(x,y) to Szudzik Pairing Value \n",
    "        \"\"\"\n",
    "        x,y = self.maze.objects.agent.positions[0]\n",
    "        return Env.pair(x,y)\n",
    "    \n",
    "    def get_agent_position(self):\n",
    "        \"\"\"\n",
    "        Simple unpair the state(Szudzik Pairing Value) to get the agent's positions\n",
    "        \"\"\"\n",
    "        return Env.unpair(self.s)\n",
    "        \n",
    "    @staticmethod\n",
    "    def pair(x,y):\n",
    "        \"\"\"\n",
    "        By using Szudzik Pairing, get the one dimentional value for x,y pair\n",
    "        \"\"\"\n",
    "        return y*y+x if y > x else x*x+x+y\n",
    "\n",
    "    @staticmethod\n",
    "    def unpair(z):\n",
    "        \"\"\"\n",
    "        Get the x,y coordinates back from the Szudzik Pairing Value\n",
    "        \"\"\"\n",
    "        q = math.floor(math.sqrt(z))\n",
    "        l = z - q ** 2\n",
    "        return (l, q) if l < q else (q,(l-q)) \n",
    "    \n",
    "    @staticmethod\n",
    "    def repeat_upsample(rgb_array, k=1, l=1, err=[]):\n",
    "        \"\"\"\n",
    "        Upscale the maze image by KxL\n",
    "        \n",
    "        Taken from https://github.com/openai/gym/issues/550\n",
    "        \"\"\"\n",
    "        # repeat kinda crashes if k/l are zero\n",
    "        if k <= 0 or l <= 0: \n",
    "            if not err: \n",
    "                print(f\"Number of repeats must be larger than 0, k: {k}, l: {l}, returning default array!\")\n",
    "                err.append('logged')\n",
    "            return rgb_array\n",
    "\n",
    "        # repeat the pixels k times along the y axis and l times along the x axis\n",
    "        # if the input image is of shape (m,n,3), the output image will be of shape (k*m, l*n, 3)\n",
    "\n",
    "        return np.repeat(np.repeat(rgb_array, k, axis=0), l, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MazeEnvironment:\n",
    "    \"\"\"\n",
    "    Initiate appropriate Maze with the provided sizes and obstacle lengths\n",
    "    \"\"\"\n",
    "    def __init__(self, size, k, obstacle_len=4):\n",
    "        self.size = size\n",
    "        self.k = k\n",
    "        self.obstacle_len = obstacle_len\n",
    "        self.env = self.generate_env()\n",
    "        \n",
    "        self.start_idx = [[1, 1]]\n",
    "        self.goal_idx = [[self.size[0]-2, self.size[1]-2]]\n",
    "        self.env_id = 'SimpleEmptyMaze-v0'\n",
    "        # Register the environment, here the max episode steps should be high \n",
    "        # so that we make sure the agent met the goal position\n",
    "        try:\n",
    "            gym.envs.register(id=self.env_id, entry_point=Env, max_episode_steps=20_000)\n",
    "        except:\n",
    "            del gym.envs.registration.registry.env_specs[self.env_id]\n",
    "            gym.envs.register(id=self.env_id, entry_point=Env, max_episode_steps=20_000)\n",
    "    \n",
    "    def get_maze(self):\n",
    "        \"\"\"\n",
    "        Get the maze where every position is defined strictly.\n",
    "        0 -> Free\n",
    "        1 -> Obstacle\n",
    "        2 -> Agent\n",
    "        3 -> Goal\n",
    "        \"\"\"\n",
    "        x = self.env\n",
    "        class Maze(BaseMaze):\n",
    "            @property\n",
    "            def size(self):\n",
    "                return x.shape\n",
    "\n",
    "            def make_objects(self):\n",
    "                free = Object('free', 0, color.free, False, np.stack(np.where(x == 0), axis=1))\n",
    "                obstacle = Object('obstacle', 1, color.obstacle, True, np.stack(np.where(x == 1), axis=1))\n",
    "                agent = Object('agent', 2, color.agent, False, [])\n",
    "                goal = Object('goal', 3, color.goal, False, [])\n",
    "                return free, obstacle, agent, goal\n",
    "        return Maze()\n",
    "    \n",
    "    def set_start(start_idx):\n",
    "        \"\"\"\n",
    "        Update start index\n",
    "        \"\"\"\n",
    "        self.start_idx = start_idx\n",
    "    \n",
    "    def set_goal(goal_idx):\n",
    "        \"\"\"\n",
    "        Update goal index\n",
    "        \"\"\"\n",
    "        self.goal_idx = goal_idx\n",
    "    \n",
    "    def generate_env(self):\n",
    "        \"\"\"\n",
    "        k -> number of obstacles\n",
    "        \"\"\"\n",
    "        # Start environment with array of Zeros where 0's are free spaces, 1's are the walls\n",
    "        arr = np.zeros(shape=self.size, dtype=int)\n",
    "        # Create Left-Right Walls\n",
    "        for i in range(self.size[0]):\n",
    "            arr[i, 0] = 1\n",
    "            arr[i, self.size[1] - 1] = 1\n",
    "\n",
    "        # Create Bottom-Up Walls\n",
    "        for i in range(self.size[1]):\n",
    "            arr[0, i] = 1\n",
    "            arr[self.size[0] - 1, i] = 1\n",
    "\n",
    "        # Now environment walls are generated now lets add obstacles\n",
    "\n",
    "        # Generate k Obstacles\n",
    "        for i in range(self.k):\n",
    "            # Choose random point to put the current obstacle\n",
    "            row = random.randrange(1, self.size[0] - 1 - self.obstacle_len)\n",
    "            column = random.randrange(1, self.size[1] - 1 - self.obstacle_len)\n",
    "\n",
    "            # Roll dice to decide whether to put the obstacle in vertical or in horizontal shape\n",
    "            isVertical = random.randint(0, 1)\n",
    "            if isVertical == 1:\n",
    "                for j in range(self.obstacle_len):\n",
    "                    arr[row, column + j] = 1\n",
    "            else:\n",
    "                for j in range(self.obstacle_len):\n",
    "                    arr[row + j, column] = 1\n",
    "\n",
    "        return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to choose the next action \n",
    "def choose_action(state, epsilon): \n",
    "    action=0\n",
    "    # Decide whether or not go random or use qtable\n",
    "    if random.uniform(0,1) < epsilon: \n",
    "        action = env.action_space.sample()              # Explore the action space\n",
    "    else:\n",
    "        action = np.argmax(q_table[state])              # Use learnt q-values\n",
    "    return action "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to learn the Q-value \n",
    "def update(q_table, alpha, gamma, state, state2, reward, action, action2): \n",
    "    predict = q_table[state, action] \n",
    "    target = reward + gamma * q_table[state2, action2] \n",
    "    q_table[state, action] = q_table[state, action] + alpha * (target - predict) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(env, q_table, n_episodes, alpha, gamma, epsilon):\n",
    "    for i in range(1, n_episodes):\n",
    "        state = env.reset()                                     # Start\n",
    "        action = choose_action(state, epsilon)\n",
    "        epochs, penalties, reward = 0, 0, 0                     # init variables\n",
    "        done = False\n",
    "        \n",
    "        while not done:                                         # while the agent has not finished\n",
    "            # Act and get respond from the environment\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            \n",
    "            next_action = choose_action(next_state, epsilon)\n",
    "            \n",
    "            # Update Q-Value of the current state\n",
    "            update(q_table, alpha, gamma, state, next_state, reward, action, next_action);\n",
    "            \n",
    "            if reward == -100:\n",
    "                penalties += 1\n",
    "            \n",
    "            state = next_state \n",
    "            action = next_action \n",
    "        \n",
    "            epochs += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(env, q_table):\n",
    "    img = env.render('rgb_array')\n",
    "    env = gym.wrappers.Monitor(env, './', force=True, uid=random.randint(0, 100))\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = np.argmax(q_table[state])\n",
    "        state, reward, done, info = env.step(action)\n",
    "    env.close()\n",
    "    f = list(Path('./').glob('*.mp4'))[0]\n",
    "    reader = imageio.get_reader(f)\n",
    "    f = f'./{maze_env.env_id}.gif'\n",
    "    with imageio.get_writer(f, fps=50) as writer:\n",
    "        [writer.append_data(img) for img in reader]\n",
    "    return Img(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1- 20 x 20 Maze With 10 Obstacles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This one has to be defined globally because of how the Env is defined.\n",
    "maze_env = MazeEnvironment((20, 20), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x223c86fb288>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = gym.make(maze_env.env_id)\n",
    "env.reset()\n",
    "img = env.render('rgb_array')\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "440"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets get the number of states for the current environment wihch is 20x20\n",
    "n_states = Env.pair(env.maze.size[0], env.maze.size[1]) # Get Maximum State Number\n",
    "n_states # The position of agent expresses the state of the board"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now we get the size of the possible actions\n",
    "n_actions = env.action_space.n \n",
    "n_actions # North, South, West, East in this order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(440, 4)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now it's time to create q_table which is a mapping of every state to every action\n",
    "q_table = np.zeros([n_states, n_actions])        # Table of n_states x n_actions\n",
    "q_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(env, q_table, 10_000, alpha=0.4, gamma=0.65, epsilon=0.35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize(env, q_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2- 20 x 20 Maze With 20 Obstacles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maze_env = MazeEnvironment((20, 20), 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env2 = gym.make(maze_env.env_id)\n",
    "env2.reset()\n",
    "img2 = env2.render('rgb_array')\n",
    "plt.imshow(img2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_table2 = np.zeros([Env.pair(env2.maze.size[0], env2.maze.size[1]), env2.action_space.n ])\n",
    "q_table2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(env2, q_table2, 10_000, alpha=0.4, gamma=0.65, epsilon=0.35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize(env2, q_table2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3- 40 x 40 Maze With 20 Obstacles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maze_env = MazeEnvironment((40, 40), 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env3 = gym.make(maze_env.env_id)\n",
    "env3.reset()\n",
    "img3 = env3.render('rgb_array')\n",
    "plt.imshow(img3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_table3 = np.zeros([Env.pair(env3.maze.size[0], env3.maze.size[1]), env3.action_space.n ])\n",
    "q_table3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(env3, q_table3, 1_000, alpha=0.4, gamma=0.65, epsilon=0.35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize(env3, q_table3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4- 40 x 40 Maze With 50 Obstacles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMaze():\n",
    "    env = gym.make(maze_env.env_id)\n",
    "    env.reset()\n",
    "    img = env.render('rgb_array')\n",
    "    plt.imshow(img)\n",
    "    q_table = np.zeros([Env.pair(env.maze.size[0], env.maze.size[1]), env.action_space.n ])\n",
    "    \n",
    "    return env, q_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maze_env = MazeEnvironment((40, 40), 50)\n",
    "env4, q_table4 = getMaze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(env4, q_table4, 1_000, alpha=0.4, gamma=0.65, epsilon=0.35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize(env4, q_table4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
